import os
import sys
import time
import torch
from concurrent.futures import ThreadPoolExecutor, as_completed
from server import FedServer
from client import FedClient
from utils import PlateDataset


def main():

    # =============== åˆå§‹åŒ–é˜¶æ®µ =============== #
    print("ğŸš€ ç³»ç»Ÿåˆå§‹åŒ–ä¸­...")
    start_time = time.time()

    # åˆå§‹åŒ–è”é‚¦å­¦ä¹ æœåŠ¡å™¨
    server = FedServer()
    print(f"âœ… æœåŠ¡å™¨åˆå§‹åŒ–å®Œæˆï¼ˆè€—æ—¶ï¼š{time.time() - start_time:.2f}sï¼‰")

    # =============== å®¢æˆ·ç«¯åˆå§‹åŒ– =============== #
    print("\nğŸš€ æ­£åœ¨åˆå§‹åŒ–å®¢æˆ·ç«¯...")
    client_init_start = time.time()

    clients = []
    with ThreadPoolExecutor(max_workers=5) as executor:
        # å¹¶è¡Œåˆ›å»ºå®¢æˆ·ç«¯å®ä¾‹
        futures = {executor.submit(FedClient, i, './processed', server): i for i in range(5)}

        for future in as_completed(futures):
            client_id = futures[future]
            try:
                client = future.result()
                clients.append(client)
                print(f"  å®¢æˆ·ç«¯ {client_id} åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                print(f"âš ï¸ å®¢æˆ·ç«¯ {client_id} åˆå§‹åŒ–å¤±è´¥: {str(e)}")

    print(f"âœ… æ‰€æœ‰å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆï¼ˆå…±è€—æ—¶ï¼š{time.time() - client_init_start:.2f}sï¼‰")

    # =============== æ¨¡å‹é¢„åŠ è½½ =============== #
    print("\nğŸ”¥ æ­£åœ¨é¢„åŠ è½½æ¨¡å‹...")
    model_load_start = time.time()

    # æ˜¾å¼åˆå§‹åŒ–æ‰€æœ‰å®¢æˆ·ç«¯çš„æ¨¡å‹
    for idx, client in enumerate(clients):
        try:
            client.initialize_models()
            print(f"  å®¢æˆ·ç«¯ {idx} æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ å®¢æˆ·ç«¯ {idx} æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            continue

    print(f"âœ… æ¨¡å‹é¢„åŠ è½½å®Œæˆï¼ˆå…±è€—æ—¶ï¼š{time.time() - model_load_start:.2f}sï¼‰")

    # =============== æ•°æ®å‡†å¤‡ =============== #
    print("\nğŸ“‚ æ­£åœ¨åŠ è½½æµ‹è¯•æ•°æ®é›†...")
    test_set = PlateDataset('./processed', mode='test')
    print(f"âœ… æµ‹è¯•é›†åŠ è½½å®Œæˆï¼ˆæ ·æœ¬æ•°ï¼š{len(test_set)}ï¼‰")

    # =============== é¢„çƒ­é˜¶æ®µ =============== #
    print("\nğŸ”¥ æ‰§è¡Œç³»ç»Ÿé¢„çƒ­...")
    warmup_start = time.time()

    if clients:
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªå®¢æˆ·ç«¯è¿›è¡Œé¢„çƒ­
        sample_client = clients[0]

        # å¹¶è¡Œæ‰§è¡Œé¢„çƒ­æ“ä½œ
        with ThreadPoolExecutor() as executor:
            futures = [
                executor.submit(lambda: sample_client.loader.dataset[0]),  # æ•°æ®åŠ è½½é¢„çƒ­
                executor.submit(  # æ¨¡å‹æ¨ç†é¢„çƒ­
                    lambda: sample_client.local_recognizer(
                        torch.randn(1, 3, 48, 168).to(sample_client.device)
                    )
                )
            ]

            for future in as_completed(futures):
                try:
                    future.result()
                    print("  é¢„çƒ­æ“ä½œå®Œæˆ")
                except Exception as e:
                    print(f"âš ï¸ é¢„çƒ­å¤±è´¥: {str(e)}")

    print(f"âœ… é¢„çƒ­å®Œæˆï¼ˆè€—æ—¶ï¼š{time.time() - warmup_start:.2f}sï¼‰")

    # =============== è®­ç»ƒé…ç½® =============== #
    num_rounds = 10
    local_epochs = 2
    model_save_dir = "models"
    os.makedirs(model_save_dir, exist_ok=True)

    # =============== è”é‚¦è®­ç»ƒå¾ªç¯ =============== #
    print(f"\nğŸ å¼€å§‹è”é‚¦è®­ç»ƒï¼ˆæ€»è½®æ¬¡ï¼š{num_rounds}ï¼‰")

    for round_idx in range(num_rounds):
        round_start = time.time()
        print(f"\n=== ç¬¬ {round_idx + 1}/{num_rounds} è½® ===")

        try:
            # =============== å®¢æˆ·ç«¯æœ¬åœ°è®­ç»ƒ =============== #
            updates = []
            print(f"âŒ› å®¢æˆ·ç«¯æœ¬åœ°è®­ç»ƒä¸­...")
            train_start = time.time()

            with ThreadPoolExecutor() as executor:
                future_to_client = {
                    executor.submit(client.local_train, local_epochs): client
                    for client in clients
                }

                for future in as_completed(future_to_client, timeout=7200):
                    client = future_to_client[future]
                    try:
                        update = future.result()
                        updates.append(update)
                        print(f"  å®¢æˆ·ç«¯ {client.client_id} è®­ç»ƒå®Œæˆ")
                    except Exception as e:
                        print(f"âš ï¸ å®¢æˆ·ç«¯ {client.client_id} è®­ç»ƒå¤±è´¥: {str(e)}")

            print(f"âœ… æœ¬åœ°è®­ç»ƒå®Œæˆï¼ˆè€—æ—¶ï¼š{time.time() - train_start:.2f}sï¼‰")

            # =============== æ¨¡å‹èšåˆ =============== #
            if updates:
                print("ğŸ”„ æ­£åœ¨èšåˆæ¨¡å‹æ›´æ–°...")
                aggregate_start = time.time()
                server.aggregate(updates)
                print(f"âœ… æ¨¡å‹èšåˆå®Œæˆï¼ˆè€—æ—¶ï¼š{time.time() - aggregate_start:.2f}sï¼‰")

                # =============== æ¨¡å‹è¯„ä¼° =============== #
                print("ğŸ“Š æ­£åœ¨è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
                eval_start = time.time()
                accuracy = server.evaluate(test_set)
                print(f"âœ… è¯„ä¼°å®Œæˆ | å‡†ç¡®ç‡ï¼š{accuracy:.2%}ï¼ˆè€—æ—¶ï¼š{time.time() - eval_start:.2f}sï¼‰")
            else:
                print("âš ï¸ æœ¬è½®æ— æœ‰æ•ˆæ›´æ–°ï¼Œè·³è¿‡èšåˆå’Œè¯„ä¼°")

            # =============== æ¨¡å‹ä¿å­˜ =============== #
            if (round_idx + 1) % 5 == 0:
                version = (round_idx + 1) // 5
                save_start = time.time()
                print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜ç¬¬ {version} ç‰ˆæ¨¡å‹...")

                detector_path = os.path.join(model_save_dir, f"detector_v{version}_round{round_idx + 1}.pt")
                recognizer_path = os.path.join(model_save_dir, f"recognizer_v{version}_round{round_idx + 1}.pt")

                torch.save(server.global_detector.state_dict(), detector_path)
                torch.save(server.global_recognizer.state_dict(), recognizer_path)
                print(f"âœ… æ¨¡å‹ä¿å­˜å®Œæˆï¼ˆè€—æ—¶ï¼š{time.time() - save_start:.2f}sï¼‰")

            # æ¯è½®ç»“æŸåæ·»åŠ 
            if torch.cuda.is_available():
                torch.cuda.synchronize()  # ç¡®ä¿CUDAæ“ä½œå®Œæˆ
                torch.cuda.empty_cache()  # æ¸…ç©ºç¼“å­˜

            # å†…å­˜ç›‘æ§
            print(f"æ˜¾å­˜çŠ¶æ€ - å·²åˆ†é…: {torch.cuda.memory_allocated() / 1e6:.2f}MB, "
                    f"ä¿ç•™: {torch.cuda.memory_reserved() / 1e6:.2f}MB")

            # æ·»åŠ å†…å­˜è¯Šæ–­
            if round_idx % 2 == 0:
                print(f"å†…å­˜çŠ¶æ€ - å·²åˆ†é…ï¼š{torch.cuda.memory_allocated() / 1e6:.2f}MB, "
                      f"ç¼“å­˜ï¼š{torch.cuda.memory_reserved() / 1e6:.2f}MB")

        except Exception as e:
            print(f"âš ï¸ ç¬¬ {round_idx + 1} è½®è®­ç»ƒå¼‚å¸¸: {str(e)}")
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        print(f"â±ï¸ æœ¬è½®æ€»è€—æ—¶ï¼š{time.time() - round_start:.2f}s")

    print("\nğŸ‰ è”é‚¦è®­ç»ƒå®Œæˆï¼")


if __name__ == "__main__":
    # =============== ç³»ç»Ÿé…ç½®ä¼˜åŒ– =============== #
    if sys.platform.startswith('win'):
        import ctypes

        ctypes.windll.kernel32.SetProcessWorkingSetSize(-1, 1 << 28, 1 << 30)  # é™åˆ¶å†…å­˜å·¥ä½œé›†
        # Windowsç³»ç»Ÿä¼˜åŒ–
        import multiprocessing

        multiprocessing.freeze_support()
        multiprocessing.set_start_method('spawn', force=True)
        print("âš ï¸ Windowsç³»ç»Ÿå·²å¯ç”¨å…¼å®¹æ¨¡å¼")
    else:
        # Linux/Macç³»ç»Ÿä¼˜åŒ–
        torch.set_num_threads(4)
        torch.backends.cudnn.benchmark = True
        print("ğŸ§ Linux/Macç³»ç»Ÿå·²å¯ç”¨åŠ é€Ÿæ¨¡å¼")

    # =============== å¯åŠ¨è®­ç»ƒ =============== #
    total_start = time.time()
    try:
        main()
    except KeyboardInterrupt:
        print("\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    finally:
        print(f"\nğŸ æ€»è¿è¡Œæ—¶é—´ï¼š{time.time() - total_start:.2f}ç§’")